# üß† Deep Learning

**üèõÔ∏è University:** Sharif University of Technology  
**üè¢ Department:** Electrical Engineering  
**üë®‚Äçüè´ Instructor:** Prof. Emad Fatemizadeh  

---

## üìò Course Overview  
This course delves into deep learning, a key subfield of machine learning that utilizes multi-layer neural networks to autonomously discover complex patterns and representations from data. It contrasts with traditional machine learning by reducing the reliance on feature engineering, allowing models to learn directly from raw data. Applications of deep learning are broad and influential, ranging from image classification and language translation to autonomous driving. This course covers the core concepts, theoretical underpinnings, and practical implementations across various domains such as computer vision, natural language processing, and speech recognition.

**Textbooks:**
- Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. "Deep Learning." MIT Press, 2016.
- Murphy, Kevin. "Probabilistic Machine Learning ‚Äì An Introduction." MIT Press, 2022.
- Theodoridis, Sergios. "Machine Learning: A Bayesian and Optimization Perspective." Academic Press, 2015.
- Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. "Mathematics for Machine Learning." Cambridge University Press, 2020.
- Petersen, Kaare B., and Michael S. Pedersen. "Matrix Cookbook." Technical University of Denmark, 2012.

---

## üìù Syllabus

### 1Ô∏è‚É£ Introductions  
- **Introduction to Machine Learning Concepts**
- **Must-Known Classical Methods**
- **Model Fitness**
- **Data Splitting**
- **Performance Measures**

### 2Ô∏è‚É£ Essential Mathematics for Machine Learning  
- **Linear Algebra**
- **Random Vectors**
- **PCA and SVD**
- **Optimization (SGD Family)**

### 3Ô∏è‚É£ Shallow Linear Classification and Regression  
- **Single Layer Perceptron (SLP)**
- **Multilayer Perceptron (MLP)**
- **Error Back Propagation (EBP) Algorithm**
- **Most Important Theorems**
- **Why Deep Not Shallow**

### 4Ô∏è‚É£ Regularization and Optimization  
- **The Bias-Variance Rethinking**
- **L1, L2, L1+L2 Regularization**
- **Data Augmentation**
- **Dropout**
- **Early Stopping**
- **Optimization Challenges**
- **Normalizations**

### 5Ô∏è‚É£ Convolutional Neural Networks (CNN)  
- **History**
- **CNN Architecture**
- **Different Types of Convolutions**
- **Learning Tricks (Pooling, etc.)**
- **Foundational Architecture (From AlexNet to MobileNet and State-of-the-Art Architecture)**

### 6Ô∏è‚É£ Application of CNN in Computer Vision  
- **Computer Vision Tasks**
- **Deep CNN in Computer Vision**
- **Segmentation (Brief)**
- **Object Detection (Brief)**

### 7Ô∏è‚É£ Sequence Modelling  
- **RNN Family**
- **Backpropagation Through Time**
- **LSTM, GRU, and their variants**
- **Introduction to Natural Language Processing (NLP)**
- **Word Embedding (word2vec: CBoW, SkipGram)**
- **Attention**
- **Self-Attention, Transformers, and their application in NLP**
- **Vision Transformer and Applications**

### 8Ô∏è‚É£ Unsupervised Learning  
- **Auto Encoder (AE), and its variants (SAE, DAE, CAE, etc.)**
- **Variational Auto Encoder (VAE) Mathematics and ELBO**
- **VAE Family (CVAE, HVAE, Œ≤-VAE, and VQ-VAE)**

### 9Ô∏è‚É£ Adversarial Learning and Diffusion Models  
- **GAN Mathematics**
- **JS (Jensen‚ÄìShannon) Divergence**
- **GAN Hardness (Gradient Vanishing and Mode Collapse)**
- **GAN Family (CGAN, DC-GAN, CycleGAN, Wasserstein-GAN, Progressive-GAN, Style-GAN)**
- **Diffusion Models**

---
